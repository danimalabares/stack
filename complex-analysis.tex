\input{preamble}

\begin{document}

\title{Complex Analysis}
\maketitle

\phantomsection
\label{section-phantom}
\hfill
\href{http://github.com/danimalabares/stack}{github.com/danimalabares/stack}

\tableofcontents

\section{Holomorphic functions}
\label{section-holomorphic-functions}

\begin{definition}
\label{definition-holomorphic-function}
A function $f:W\subset \mathbb{C} \to \mathbb{C}$ is {\it holomorphic} at $z_0
\in U$ if
$$
\lim_{z\to z_0} \frac{f(z_0-z)-f(z_0)}{z-z_0}
$$
exists. This is equivalent to
$$
\lim_{h\to 0} \frac{f(z_0+h)-f(z_0)}{h}
$$
where $h$ is a complex parameter.
\end{definition}

\section{Cauchy-Riemann equations}
\label{section-Cauchy-Riemann-equations}

\begin{theorem}
\label{theorem-Cauchy-Riemann}
Let $f:W\subset\mathbb{C}\to \mathbb{C}$ be a function. Let $z=x+iy$ coordinates
in $\mathbb{C}$. Define $f:=u+iv$ and
$z_0=x_0+iy_0 \in U$. $f$ is holomorphic at $z_0$ if and only if
$$
\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y},\qquad \frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}
$$
\end{theorem}

\section{Integration}
\label{section-integration}

The complex integram may first be defined for a complex-valued function $f$
defined on a real integral as
$$
\int_a^bf=\int_a^b u + i \int_a^b
$$
And then for curves as
$$
\int_\gamma f:=\int_a^b f(\gamma(t))\gamma'(t)dt
$$
A natural question is: what's the essential difference between complex and real
analysis? What is the power behind Cauchy-Riemann equations?

Recall
\begin{theorem}[Stokes]
\label{theorem-Stokes}
\begin{reference}
\cite[Chapter 4, Theorem 1]{ahl}
\end{reference}
The line integral $\int_\gamma p dx +q dy$ defined in $\Omega$ depends on the
end points of $\gamma$ if and only if there exists a function $U(x,y)$ in
$\Omega$ with the partial derivatives $\partial U/\partial x=p$ and 
$\partial U/\partial y=q$, that is, if $pdx +qdy$ is exact.
\end{theorem}

\begin{proof}
Fundamental theorem of calculus.
\end{proof}
So, when computing a complex integral $\int_\gamma f$, we are interested in
knowing when is the form $fdz$ exact. Which says that there is a function $F$
such that $\partial_x F=f$ and $\partial_y F=if$. In fact, this is the same as
asking that $F$ satisfies the Cauchy-Riemann equations (provided $f$ is
continuous).

Thus, the integral $\int_\gamma f$ for continuous $f$ depends only on the end
points of $\gamma$ if and only if $f$ is the derivative of an analytic function
in $\Omega$.

\medskip\noindent

The theory of complex integration has to do with homology. The following
result does not hold for domains that are not simply connected:

\begin{theorem}[Cauchy]
\label{theorem-Cauchy}
If $f(z)$ is analytic in an open disk $\Delta$, then
\begin{equation}
\label{equation-Cauchy-theorem}
\int_\gamma f(z)dz=0
\end{equation}
for every closed curve $\gamma$ in $\Delta$.
\end{theorem}

\begin{proof}
I prove this using Stokes theorem. The fact that $f$ is holomorphic says that
$\overline{\partial}f=0$. In turn, $d=\partial+\overline{\partial}$, so that
$df=\partial f$, which is a $(1,0)$-form (just as $\overline{\partial}$ maps
functions to $(0,1)$-forms… (must justify…)). This makes $d(fdz)=df\wedge dz$ a
$(2,0)$-form, but there are no such forms on $\mathbb{C}$ (because there are no
holomorphic or antiholomorphic 2-forms on a complex dimension 1 space).
\end{proof}

The following limit condition allows for the former result even if the domain
has some points removed:

\begin{theorem}
\label{theorem-Cauchy-removable}
Let $f$ be analytic in the region $\Delta'$ by omitting a finite number of
points $\zeta_j$ from an open disk $\Delta$. If $f$ satisfies that
\begin{equation}
\label{equation-removable-singularity}
\lim_{z\to\zeta_j} (z-\zeta_j)f(z)=0
\end{equation}
for all $j$, then Eq. \ref{equation-Cauchy-theorem} holds for any closed curve
$\gamma$ in $\Delta'$.
\end{theorem}

\begin{proof}[Sketch of proof]
It suffices to prove that Eq. \ref{equation-removable-singularity} implies that
we can extend $f$ to a holomorphic function on all of $\Delta$ and apply Stokes
as in Theorem \ref{theorem-Cauchy}. But that won't work: for that we need Cauchy
integral formula, and that's why I'm here: to prove Cauchy integral formula.

So I think it might be using logarithmic derivative. The limit above allows to
write
$$
|f(z)|\leq \frac{\varepsilon}{|z-\zeta_j|}
$$
Then integrate. On the left we can bound the integral of $f$ after applying
integral triangle inequality, and on the right we have logarithmic derivative of
$z-\zeta_j$. This will be a fixed number (the index, see Definition
\ref{definition-index}), so that we have effectively bounded the integral.
\end{proof}

\section{Cauchy integral formula}
\label{section-Cauchy-integral-formula}

As we shall see, Cauchy integral formula is a straightforward consequence of
Theorem \ref{theorem-Cauchy-removable}. It says that we can compute the value of
a holomorphic function at a point as an integral around the point.

To prove a very general version we shall use the notion of index of a curve
about a point, which tells us how many times the curve winds about the point.

\begin{lemma}
\label{lemma-index-is-multiple-of-2pii}
\begin{reference}
\cite[Section 2.2, Lemma 1]{ahl}
\end{reference}
If the piecewise differentiable closed curve $\gamma$ does not pass through the
point $a$, then the value of the integral
$$
\int_\gamma\frac{dz}{z-a}
$$
is a multple of $2\pi i$.
\end{lemma}

\begin{proof}
We are tempted to simply write the integrand as the logarithmic derivative of
the function $f(z)=z-a$. But this isn't quite right, we must be careful with the
domain of the logarithm. But it is instructive to see the computation:
$$
\int_\gamma\frac{dz}{z-a}=\int_\gamma d \text{log}(z-a)=
\int_\gamma d\text{log}|z-a|+i\int_\gamma d \text{arg}(z-a)
$$
If  $\gamma$ is closed then $\text{log}|z-a|$ would return to its initial value
and $\text{arg}(z-a)$ increases or decreases by a multiple of $2\pi$.
\end{proof}

\begin{definition}
\label{definition-index}
The {\it index} of the point $a$ with respect to the closed curve $\gamma$ is
\begin{equation}
\label{equation-index}
n(\gamma,a):=\frac{1}{2\pi i}\int_\gamma\frac{dz}{z-a}
\end{equation}
\end{definition}

\begin{theorem}[Cauchy Integral Formula]
\label{theorem-Cauchy-integral-formula}
\begin{reference}
\cite[Section 2.2, Theorem 6]{ahl}
\end{reference}
Suppose that $f(z)$ is analytic in an open disk $\Delta$, and let $\gamma$ be a
closed curve in $\Delta$. For any point not on $\gamma$,
\begin{equation}
\label{equation-Cauchy-formula-with-index}
n(\gamma,a)f(a)=\frac{1}{2\pi i}\int_\gamma\frac{f(z)dz}{z-a}
\end{equation}
where $n(\gamma,a)$ is the index of $a$ with respect to $\gamma$.
\end{theorem}

\begin{proof}
This is a simple application of Theorem \ref{theorem-Cauchy-removable} for the
function
$$
F(z):=\frac{f(z)-f(a)}{z-a}
$$
Notice the limit condition holds, so that the integral vanishes!
\end{proof}

Notice that we can differentiate Cauchy integral formula to obtain
\begin{equation}
\label{equation-first-derivative-Cauchy-formula}
f'(z)=\frac{1}{2\pi i}\int_\gamma\frac{f(\zeta)}{(\zeta-z)^2}d\zeta
\end{equation}
and more generally
$$
f^{(n)}(z)=\frac{n!}{2\pi i}\int_\gamma \frac{f(\zeta)}{(\zeta-z)^{n+1}}
$$
A technical lemma is used by Ahlfors to confirm that indeed we can differentiate
under the integral sign.

\begin{lemma}
\label{lemma-technical-lemma}
[missing]
\end{lemma}

The importance of this result is that it shows that a holomorphic function has
derivatives of all degrees.

Let us state

\begin{theorem}[Morera]
\label{theorem-Morera}
\begin{reference}
\cite[p. 122]{ahl}
\end{reference}
If $f(z)$ is defined and continuous in a region $\Omega$, and if 
$\int_\gamma fdz=0$ for all closed curves $\gamma$ in $\Omega$, then $f(z)$ is
analytic in $\Omega$.
\end{theorem}

This is what Lee calls a {\it conservative covector field}, i.e. a form whose
integral on closed curves vanishes. The proof of Morera's theorem then reduces
to the fact that a covector field is conservative if and only if it is exact
\cite[Theorem 11.42]{les}. The reverse implication is the fundamental theorem of
calculus. The forward implication is not very straightforward in \cite{les}.

As a final remark, I add that (\cite[Propoistion 11.40]{les}) a smooth covector
field is conservative if and only if its line integrals are path-independent, in
the sense that integrals coincide along two {\it piecewise} smooth curve
segments with the same starting and ending points. That is, form is exact
implies integral independent of homotopy class?

We finish with

\begin{theorem}[Liouville]
\label{theorem-Liouville}
A bounded holomorphic function defined on all of $\mathbb{C}$ must be constant.
\end{theorem}

\begin{proof}
Let $\gamma$ be a circle of radius $r$ about $z$, then by the first derivative
of Cauchy formula (Eq. \ref{equation-first-derivative-Cauchy-formula}),
$$
|f'(z)|\leq \frac{M}{2\pi i}\int_\gamma \frac{d\zeta}{|\zeta-z|^2}
$$
and compute the integral, of course is $\frac{2\pi i}{r}$.
\end{proof}

A fun application of this is proving the fundamental theorem of algebra. If a
complex polynomial had no zeroes, $1/P(z)$ would be analytic in all of
$\mathbb{C}$, and since $P(z)$ tends to $\infty$ as $z$ tends to $\infty$,
$1/P(z)$ is bounded (use Riemann sphere argument).

\section{Removable singularity}
\label{section-removable-singularity}

The following theorem is one of those examples where there should be no theorem.
The proof if just looking at Cauchy integral formula and noticing that the
function defined at the singularity via the integral around it is holomorphic.

\begin{theorem}[Removable singularity theorem]
\label{theorem-removable-singularity}
Let $f(z)$ be analytic in a region $\Omega'$ obtained by omitting a point $z_0$
from a region $\Omega$. There exists an analytic function  defined on all 
$\Omega$ that coincides with $f$ on $\Omega'$ if and only if 
$\lim_{z \to z_0} (z-z_0)f(z)=0$. Such a function is unique.
\end{theorem}

Applying this fact any finite number of times to the function
$$
F(z)=\frac{f(z)-f(z_0)}{z-z_0},
$$
which has a removable singularity at $z_0$, we obtain the first result of the 
next section.

\section{Zeroes of a holomorphic function}
\label{section-zeroes}

\begin{lemma}
\label{lemma-finite-Taylor-expansion}
If $f$ is analytic in a connected open set $W\subset\mathbb{C}$ and $a \in W$,
then we can write
$$
f(z)=f(a)+f'(a)(z-a)+\frac{f''(a)}{2!}(z-a)^2+\ldots
+\frac{f^{(n-1)}(a)}{(n-1)!}(z-a)^{n-1}+f_n(z)(z-a)^n
$$
for some function $f_n$ analytic on $W$.
\end{lemma}

\begin{theorem}
\label{theorem-zeroes-are-isolated-and-have-finite-order}
If $f:W\subset\mathbb{C}\to \mathbb{C}$ is a holomorphic function defined on an
open set $W$ and $f(a)=0$ for some $a\in W$, and $f$ is not identically zero,
then there is a disk $D_r(a)\subseteq W$ such that $f(z)\neq 0$ for $z \in
D_r(a)\setminus\{a\}$ and a positive integer $m$ called the {\it order} or
{\it multiplicity} of the zero $a$ such that  $f(z)=(z-a)^mh(z)$ for some
holomorphic function $h$ that does not vanish at $a$. The order of a zero is
equal to the smallest integer $m$ such that $f^{(m)}(a)\neq 0$.
\end{theorem}

\begin{proof}
If $f$ is not identically zero, there must exist a first derivative $f^{
(h)}(a)$ that is not zero, since otherwise $f$ would vanish identically in $W$.
This follows from … 

Then by Lemma \ref{lemma-finite-Taylor-expansion} we obtain that
$f(z)=f_h(z-a)^n$.
\end{proof}

\section{Argument Principle}
\label{section-argument-principle}

A simple version of the Argument Principle does not need Residue theorem:

\begin{exercise}
\label{exercise-argument-principle}
Let $f$ be a holomorphic function on a disk, non-zero on $\partial \Delta$, and
let $S_k(f):=\frac{1}{2\pi\sqrt{-1}}\int_{\partial\Delta}\frac{f'}{f}z^kdz$.
Prove the $S_k(f)=\sum d_i\alpha_i^k$ where $\alpha_i$ are all zeroes of $f$ and
$d_i$ their multiplicities.
\end{exercise}

\begin{proof}
We use Lemma \ref{lemma-finite-Taylor-expansion} to write
$f(z)=(z-z_1)h_1(z)$ where $z_1$ is a zero of $f$. Applying that to $h_1$ for
another zero  $z_2$ of $f$ we obtain 
$f(z)=(z-z_1)(z-z_2)\ldots(z-z_n)h(z)$. Computing the quotient we obtain
$$
\frac{f'(z)}{f(z)}=\frac{\text{derivative of
$(z-z_1)\ldots(z-z_n)$}}{(z-z_1)\ldots(z-z_n)}+\frac{h'(z)}{h(z)}
$$
The right hand term will vanish upon integration since it is a holomorphic
function (with no poles because $h(z)\neq 0$). The left-hand term will give a
sum of $\frac{1}{z-z_i}$ upon differentiation. As it is, the result of
integration is the sum of the orders of each zero i.e. the number of zeroes
counted without multiplicity.

Multiplying by $z^k$ yields the desired result by Cauchy formula.
\end{proof}

\begin{theorem}[Argument Principle]
\label{theorem-argument-principle-and-Rouche-theorem}
\begin{reference}
\cite[Chapter 5, Theorem 18]{ahl}
\end{reference}
If $f$ is meromorphic in $\Omega$ with zeros $a_j$ and poles $b_k$, then
\begin{equation}
\label{equation-argument-principle}
\frac{1}{2\pi i}\int_\gamma\frac{f'(\zeta)}{f(\zeta)}d\zeta
=\sum_{i}n(\gamma,a_i)-\sum_{k}n(\gamma,b_k)
\end{equation}
\end{theorem}

\begin{lemma}[Rouché theorem]
\label{lemma-Rouche-theorem}
\begin{reference}
\cite[Chapter 5, Corollary, p. 153]{ahl}
\end{reference}
Let $\gamma$ be homologous to zero in $\Omega$ and such that $n(\gamma,z)$ is
either 0 or 1 for any point $z$ not on $\gamma$. Suppose that $f$ and $g$ are
analytic in $\Omega$ and satisfy that $|f-g|<|f|$ on $\gamma$. Then $f$ and $g$
have the same number of zeros enclosed by $\gamma$.
\end{lemma}

Compare with Misha's version

\begin{theorem}[Rouché theorem]
\label{theorem-Rouche-theorem-Mishas-version}
Let $f_t$ be a family of holomorphic functions on a disk  $\Delta$, continuously
depending on a parameter $t\in \mathbb{R}$ and non-zero everywhere on its
boundary $\partial\Delta$. Prove that the number of zeros of $f_t$ in $\Delta$
is constant.
\end{theorem}

\begin{proof}
Consider the map $t\mapsto f_t\mapsto S(f_t)$, which is continuous by hypothesis
and because $S$, being an integral, is continuous. Then we obtain a continuous
map $\mathbb{R}\to\mathbb{R}$ with integer values, meaning it must be constant.
\end{proof}

A similar argument may be used to prove that a holomorphic function $F$ defined
on $\Delta\times\Delta$ gives a holomorphic map 
\begin{equation}
\label{equation-zeros-on-polydisk}
y_0\mapsto \int_{\partial\Delta}\frac{F'(x,y_0)}{F(x,y_0)}\phi(x)dx
=\sum_id_i\phi(\alpha_i)
\end{equation}
for any holomorphic function $\phi:\Delta\to\mathbb{C}$.

\section{Holomorphic functions in several variables}
\label{section-holomorphic-functions-in-several-variables}

\begin{lemma}
\label{lemma-holomorphic-function-characterization}
\cite{lec}, Theorem 1.21. Let $U\subseteq\mathbb{C}^n$ be open and $f:U\to
\mathbb{C}$. The following are equivalent:
\begin{enumerate}
\item $f$ is holomorphic (i.e. it is continuous and has a complex partial
derivative with respect to each variable at each point of $U$)
\item $f$ is smooth and satisfies the following Cauchy-Riemann equations:
\begin{equation}
\label{equation-Cauchy-Riemann-several-variables}
\frac{\partial u}{\partial x^j}=\frac{\partial v}{\partial y^j},\qquad 
\frac{\partial u}{\partial y^j}=-\frac{\partial v}{\partial x^j}
\end{equation}
where $z^j=x^j+\sqrt{-1}y^j$ and $f(s)=u(z)+\sqrt{-1}v(x)$.
\item For each $p=(p^1,\ldots,p^n)\in U$ there exists a neighbourhood of $p$ in
$U$ on which $f$ is equal to the sum of an absolutely convergent power series of
the form
\begin{equation}
\label{equation-Taylor-series-several-variables}
f(z)=\sum_{i_1,\ldots,i_n}a_{i_1,\ldots,i_n}(z^1-p^1)\ldots(z^n-p^n)
\end{equation}
\end{enumerate}
\end{lemma}

\begin{proof}
I will prove that if $f$ is holomorphic then it has a Taylor series for $n=2$. 
First apply Cauchy integral formula on each variable to obtain
$$
f(z^1,z^2)=\frac{1}{(2\pi\sqrt{-1})^2}
\int_{\substack{|z^1-w^1|=r \\ |z^2-w^2|=r}}
\frac{f(w^1,w^2)}{(w^1-z^1)(w^2-z^2)}dw^1dw^2
$$
Now observe:
\begin{equation}
\label{equation-multivariable-Cauchy}
\frac{1}{w^1-z^1}=\frac{1}{w^1-p^1+p^1-z^1}
=\frac{1}{w^1-p^1}\frac{1}{1-\frac{p^1-z^1}{w^1-p^1}}
\end{equation}
And on the right-hand-side we have a geometric series so that we may write
$$
\frac{1}{w^1-z^1}
=\frac{1}{w^1-p^1}\sum_{k=0}^\infty\left(\frac{p^1-z^1}{w^1-p^1}\right)^k
$$
finally substituting this into (\ref{equation-multivariable-Cauchy}) we may take
the products $(p^1-z^1)^{k_1}(p^2-z^2)^{k_2}$ out of the integral and define the
remaining term as $a_{k_1k_2}$.
\end{proof}

\section{Taylor series in several variables}
\label{section-Taylor-series-in-several-variables}

\section{Identity theorem in several variables}
\label{section-identity-theorem-in-several-variables}

\begin{theorem}[Identity theorem]
\label{theorem-identity-several-variables}
\begin{reference}
\cite[Proposition 1.28]{lec}
\end{reference}
If two holomorphic functions $f,g:W\subset \mathbb{C}^n \to \mathbb{C}$ coincide
in an open subset of the connected open set $W$, 
then they coincide in all of $W$.
\end{theorem}

\begin{proof}
Let $U$ be the set where the function $h:=f-g$ and all its partial derivatives
vanish. Then $U$ is open since for every point in $U$ there is a neighbourhood
where $h$ is expressed as a Taylor series, whose coefficients must be given by
the partial derivatives of $h$. By definition of $U$, we see that $h$ must be
zero in such a neighbourhood. $U$ is also closed by continuity of partial
derivatives of all orders. 
\end{proof}

\section{Germs of holomorphic functions}
\label{section-germs-of-holomorphic-functions}

\begin{definition}
\label{definition-germ-of-holomorphic-function}
Let $U,U' \subset \mathbb{C}^n$ be neighbourhoods of $0$ and $f \in
\mathcal{O}_U$, $f'\in\mathcal{O}_{U'}$ holomorphic functions. We say that $f$
and $f'$ have the same germ, $f\sim f'$, if $f|_{U\cap U'}=f'|_{U\cap U'}$.
Clearly (?), $\sim$ gives an equivalence relation on the set of pairs $(U\ni 0,
f\in \mathcal{O}_U)$. An equivalence class is called {\bf germ of a holomorphic
function}. The space of germs in $0$ of holomorphic functions on $\mathbb{C}^n$
is denoted $\mathcal{O}_n$.
\end{definition}

\begin{exercise}
\label{exercise-stalk-is-not-finitely-generated-over-C}
Prova that the ring $\mathcal{O}_n$ of germs of holomorphic functions is not
finitely generated over $\mathbb{C}$ for any $n>0$.
\end{exercise}

\begin{proof}
I think the existence of $e^z$ as a holomorphic function satisfying the
differential equation $f'=f$ is enough to show that the coefficients of its
Taylor polynomial are all nonzero. This argument works for several variables as
well.
\end{proof}

\begin{definition}
\label{definition-formal-power-series}
A {\it formal power series} in the variables $t_1,\ldots,t_n$ is a sum
$\sum_{i=0}^{\infty}P_i(t_1,\ldots,t_n)$ where $P_i$ are homogeneous polynomials
of degree $i$. Addition of power series is defined componentwise, and
multiplication is defined via
$$
\left(\sum_{i=0}^\infty P_i(t_1,\ldots,t_n)\right)
\left(\sum_{i=0}^\infty Q_i(t_1,\ldots,t_n)\right)
=\sum_{i=0}^\infty R_i(t_1,\ldots,t_n)
$$
where $R_d(t_1,\ldots,t_n)=\sum_{i+j=d}P_i(t_1,\ldots,t_n)Q_j(t_1,\ldots,t_n)$.
\end{definition}

We can think of germs of functions in $\mathcal{O}_n$ as elements in the ring of
power series $\mathbb{C}[\![t_1,\ldots,t_n]\!]$. I think there is no problem to
prove this statement, nor the fact that power series is actually a ring with
units the nonzero constants and zero the zero constant.

\begin{exercise}
\label{exercise-stalk-has-no-zero-divisors}
Prove that $\mathcal{O}_n$ has no zero divisors.
\end{exercise}

\begin{proof}
Suppose that $PQ=0$ but neither of $P$ nor $Q$ are zero. Then $P_i \neq 0$ for
some $i$ and $Q_j \neq 0$ for some $j$. Then we can write
$$
P_iQ_j=-\sum_{\substack{p+q=i+j \\ p\neq i,q\neq j}}P_pQ_q
$$
And then what. Other way is by induction. For $n=0$ we are the complex numbers
so no problem. Suppose that $\mathcal{O}_n$ has no zero divisors. Then it looks
like we can deal with degrees smaller than $n+1$, but the $d$-th term is not a
simple product of $P_iQ_j$ with $i+j=d$, but a sum. So not sure too.
\end{proof}

\begin{definition}
\label{definition-local-ring}
A ring $R$ is called {\it local} if it contains an ideal $I\subset R$ such that
all elements $r \not \in I$ are invertible.
\end{definition}

It is an easy exercise to show that this definition is equivalent to having a
unique maximal ideal.

\begin{exercise}
\label{exercise-power-series-is-not-finitely-generated-over-stalk}
Prove that the ring $\mathbb{C}[\![ t_1,\ldots,t_n]\!]$ is not
finitely generated over $\mathcal{O}_n \subset 
\mathbb{C}[\![ t_1,\ldots,t_n]\!]$.
\end{exercise}

\begin{proof}
I thought that $\mathcal{O}_n$ would be the same as 
$\mathbb{C}[\![ t_1,..,t_n]\!]$… the natural map is surely injective
but why not surjective? There are power series that are not holomorphic
functions? Maybe because of radius of convergence? No, because germs of
holomorphic functions can be defined very near the origin… Every power series
has a nonzero radius of convergence, right?
\end{proof}

\subsection{Zeroes of holomorphic functions of several variables}
\label{subsection-zeroes-of-holomorphic-functions-of-several-variables}

\begin{definition}
\label{definition-zero-of-holomorphic-function-of-several-variables}
Let  $f\in \mathcal{O}_n$ be a germ of holomorphic function on $\mathbb{C}^n$.
Write its Taylor series $f(z)=\sum_{i=0}^\infty P_i(t_1,\ldots,t_n)$, where
$P_i$ are homogeneous polynomials of degree $i$. We say that $f$ has a {\it
zero of order (or multiplicity) $k$ in $0$} if $P_0=\ldots=P_{k-1}=0$. In this
situation {\it principal part} of the function $f$ is the homogeneous polynomial
$P_k$.
\end{definition}

The following exercise shows that a holomorphic change of coordinates will
preserve the order of a zero, and the principal part of the new function will be
determined by the differential of the change of coordinate map. We will need to
change coordinates when we do Weierstrass Preparation theorem
\ref{theorem-Weierstrass-preparation}.

\begin{exercise}
\label{exercise-principal-part-under-coordinate-change}
Let $\Phi(t_1,\ldots,t_n)=(F_1(t_1,\ldots,t_n),\ldots,F_n(t_1,\ldots,t_n))$ be
the holomorphic coordinate change around zero (?), with $F_i(0,\ldots,0)=0$ and
$A:=\left(\frac{\partial F_i}{\partial t_j}\right)_0$ its differential (I suppose
$\det A \neq 0$. Prove that
\begin{enumerate}
\item For any germ $f\in \mathcal{O}_n$ which has multiplicity $k$, the function
$\Phi^*(f)$ has zero of the same multiplicity.
\item The principal part of $\Phi^*(f)$ is obtained from the principal part of
$f$ by action of $A$.
\end{enumerate}
\end{exercise}

\begin{proof}[Sketch of proof]
\begin{enumerate}
\item The condition that $\det A \neq 0$ implies that all $F_i$ must have linear
term---otherwise their partial derivatives would vanish when evaluated at zero.
When substituting $f(z_1,\ldots,z_n)=\sum_{|\alpha|\geq k}\alpha z^\alpha$ with
$\Phi$ we find that there must be a term of order $k$.
\item The case for $n=1$ is clear. The general case follows, I think, from the
observation that the derivative $A$ recovers the linear terms, which, as shown
in the previous item, correspond to the principal part of $\Phi^*f$.
\end{enumerate}
\end{proof}

\begin{exercise}
\label{exercise-zero-locus-of-homogeneous-polynomial}
Let $Q$ be a non-zero homogeneous polynomial on $t_0,\ldots,t_n$, and $V(Q)$ its
zero set, which we consider as a subset in $\mathbb{C}P^{n}$.
\begin{enumerate}
\item Prove that $\mathbb{C}P^{n}\setminus V(Q)$ is non-empty.
\item Prove that $V(Q)\subset\mathbb{C}P^{n}$ is a set of measure 0.
\end{enumerate}
\end{exercise}

\begin{proof}
\begin{enumerate}
\item This follows from Identity Theorem
\ref{theorem-identity-several-variables}. Indeed, if $V(Q)$ was all of
$\mathbb{C}^{n}$, it would vanish on an open set, implying that $Q$ is
identically zero.
\item $V(Q)$ may be decomposed in the sets of regular and singular points.
Regular points have submanifold charts, while singular points have measure zero
by Sard's theorem.
\end{enumerate}
\end{proof}

\begin{exercise}
\label{exercise-principal-parts}
Let $f_1,f_2,\ldots\in \mathcal{O}_n$ be a countable collection of germs, which
vanish with multiplicity $k_1,k_2,\ldots$. Prove that there exists a coordinate
system $z_1,\ldots,z_n$ such that 
$\lim_{z_n\to 0} \frac{f_i(0,z_n)}{z_n^{k_i}}\neq 0$ for all $i$.
\end{exercise}

\begin{proof}[Sketch of proof]
I don't understand this exercise: evaluating any germ $f_i$ on $(0,z_n)$ will
make all terms that have any variable other than $z_n$ vanish. Thus the first
term will be a homogeneous polynomial in $z_n$, which is the principal part of
$f_i$ evaluated in $(0,z_n)$. But of course taking quotient by $z_n$ all terms
with powers of $z_n$ higher than 1 will vanish, barely leaving the principal
part of $f_i$ evaluated at $(0,1)$. But this works regardless of the coordinate
system.
\end{proof}

\section{Elementary symmetric polynomials and Newton formula}
\label{section-elementary-symmetric-polynomials-and-Newton-formula}

We work in the ring $\mathbb{Z}[\alpha_1,\ldots,\alpha_n]$ and define three
types of polynomials in this ring.

\begin{definition}
\label{definition-symmetric-polynomial}
Let $e_i \in \mathbb{Z}[\alpha_1,\ldots,\alpha_n]$ be the coefficients of the
polynomial $\prod_{i=1}^n(1+\alpha_i)$. Then $e_i$ are called {\it elementary
symmetric polynomials} on $\alpha_i$.
\end{definition}

\begin{definition}
\label{definition-Newton-polynomial}
A {\it Newton polynomial} is $p_j:=\sum_{i=1}^n\alpha_i^j$.
\end{definition}

\begin{definition}
\label{definition-complete-homogeneous-symmetric-polynomial}
A {\it complete homogeneous symmetric polynomial} of degree $k$ is $h_k$
obtained as a sum of all homogeneous monomials of degree $k$, that is,
$\alpha_1^k+\ldots+\alpha_n^k+\alpha_1^{k-1}\alpha_2+\ldots$.
\end{definition}

\begin{slogan}
\begin{reference}
\cite[p. 1]{generatingfunctionology}
\end{reference}
A generating function is a clothesline on which we hang up a sequence of numbers
for display.
\end{slogan}

Corresponding to the above definitions we have the generating functions 
$E(t):=\sum_{i=0}^ne_it^i$, $P(t):=\sum_{i=1}^\infty p_it^i$ and 
$H(t):=\sum_{i=0}^\infty h_it^i$ which are formal series in 
$\mathbb{Z}[\alpha_1,\ldots,\alpha_n][\![t]\!]$.

\begin{exercise}
\label{exercise-H}
Prove that $H(t)=\prod_{i=1}^n\frac{1}{1-t\alpha_i}$.
\end{exercise}

\begin{proof}
Let us write (possibly as a formal definition)
$$
\prod_{i=1}^n\frac{1}{1-t\alpha_i}=\prod_{i=1}^n\sum_{k=0}^\infty\alpha_i^kt^k.
$$

We also write 
$f_i\overset{\text{ops}}{\longleftrightarrow}\{\alpha_i^k\}_{k=1}^\infty$ to mean
that $f_i$ is the power series associated to the sequence
$\{\alpha_i^k\}_{k=1}^\infty$. Then the equation above is the product of the
$f_i$. Then all we have to prove is that
$$
\prod_{i=1}^nf_i\overset{\text{ops}}{\longleftrightarrow}
\left\{\sum_{i_1+\ldots+i_n=k}\alpha_1^{i_1}\ldots\alpha_n^{i_n}\right\}
_{k=1}^\infty=\{h_k\}_{k=0}^\infty
$$
This is just a generalization of the formula for product of power series for a
product of $n$ power series.
\end{proof}

\begin{exercise}
\label{exercise-E}
Prove that $E(t)=\prod_{i=1}^n(1+t\alpha_i)$.
\end{exercise}

\begin{proof}
This is just writing $\prod_{i=1}^n(1+t\alpha_i)
=\prod_{i=1}^nt(\frac{1}{t}+\alpha_i)$ and continue until we get to $E(t)$.
\end{proof}

It follows from the two previous exercises that $H(t)E(-t)=1$. Using Exercise
\ref{exercise-E} and applying logarithm we obtain that
$\frac{E'(-t)}{E(-t)}=-\sum_{i=1}^n \frac{\alpha_i}{1-t\alpha_i}$. Expanding
this formula as geometric power series we obtain that 
\begin{equation}
\label{equation-P-in-terms-of-E}
P(t)=-\frac{E'(-t)}{E(-t)}
\end{equation}

\begin{exercise}
\label{exercise-p-polynomials-of-e}
Prove that $p_i$ can be expressed as polynomials of $e_i$ (with integer
coefficients).
\end{exercise}

\begin{proof}
Using Eq. \ref{equation-P-in-terms-of-E}, and $H(t)E(-t)=1$, we have
$$
P(t)=E'(-t)H(t).
$$
Expanding the power series we obtain that the $k$-th term is
$$
p_k=\sum_{i=0}^k(-1)^{i+1}ie_ih_{k-i}
$$
\end{proof}

\begin{exercise}
\label{exercise-h-and-e}
Prove that $h_i$ can be expressed as polynomials of $e_i$ with integer
coefficients. Prove that $e_i$ can be expressed as polynomials of $h_i$ with
integer coefficients.
\end{exercise}

\begin{proof}[Sketch of proof]
By $H(t)E(-t)=1$ we see that $\frac{E'(-t)}{E(-t)}=\frac{H(t)}{H'(t)}$. Both
denominators are expressed as power series in $\alpha_i$. Multiplying by $E(-t)$
as in Exercise \ref{exercise-E} 
would let $E'(-t)$ be expressed as a power series in $h_i$ and
$\alpha_i$, while multiplying by $H(t)$ as in Exercise \ref{exercise-H} would 
let $H'(t)$ expressed in terms of
$e_i$ and $\alpha_i$.
\end{proof}

\begin{exercise}[Newton formula]
\label{exercise-Newton-formula}
Prove that $ke_k=\sum_{i=1}^{k-1}(-1)^ie_{k-i}p_i$.
\end{exercise}

\begin{proof}
Note that
$$
P(t)E(-t)=\left(\sum_{k=0}^{\infty} p_it^i\right)
\left(\sum_{i=0}^n(-1)^ie_it^i\right)
=\sum_{k=0}^\infty \sum_{i=0}^k (-1)^{k-i}e_{k-i}p_it^k
$$
using the product formula of power series, where we define $E(-t)$ as a power
series by letting $e_i=0$ for $i>n$. By Eq. \ref{equation-P-in-terms-of-E}, this
equals $E'(-t)$, which we may also see as a power series. Comparing the $k$-th
term yields the result modulo a minus sign.
\end{proof}

Finally,

\begin{exercise}
\label{exercise-symmetric-polynomials-in-terms-of-Newton-polynomials}
Prove that $e_i$ are expressed as polynomials on $p_i$ with rational
coefficients.
\end{exercise}

\begin{proof}

\end{proof}

\section{Weierstrass preparation theorem}
\label{section-Weierstrass-preparation-theorem}

We denote by $B_r(z_1,\ldots,z_{n-1})$ the ball of radius $r$ in
$\mathbb{C}^{n-1}\subset\mathbb{C}^n$.

\begin{exercise}
\label{exercise-polydisks}
Let $F$ be a holomorphic function on a neighbourhood of $0$ in $\mathbb{C}^n$,
such that $\lim_{z_n\to 0} \frac{F(0,z_n)}{z_n^k}\neq 0,\infty$.
Consider the projection map $\Pi:\mathbb{C}^n\to\mathbb{C}^{n-1}$ 
$(z_1,\ldots,z_n)\mapsto (z_1,\ldots,z_{n-1})$.
\begin{enumerate}
\item Prove that for an appropriate pair $r,r'$, the resitriction of $F$ to the
polydisk $\Delta(n-1,1):=B_r(z_1,\ldots,z_{n-1})\times\Delta_{r'}(z_n)$ nowhere
vanishes on the set $\Pi^{-1}(\partial\Delta_{r'}(z_n))$.
\item Prove that in this case the resitriction of $F$ to this polydisk has
precisely $k$ zeroes $\alpha_1,\ldots,\alpha_k$ on each fiber of $\Pi$.
\item Prove that $\sum_{i=1}^k \alpha_i^d$ is a holomorphic function on
$B_r(z_1,\ldots,z_{n-1})$.
\item Prove that any elementary symmetric polynomial on $\alpha_i$ gives a
holomorphic function on $B_r(z_1,\ldots,z_{n-1})$.
\end{enumerate}
\end{exercise}

\begin{proof}
\begin{enumerate}
\item Suppose that for every $(r,r')$ there are points 
$z_1\in B_r(z_1,\ldots,z_{n-1})$ and $z_n\in \partial\Delta_{r'}(z_n)$
where $F$ vanishes. Then we obtain a convergent sequence where $F$
vanishes, and by Identity Principle \ref{theorem-identity-several-variables} we
conclude that $F$ mus be identically zero.
\item This is just applying Argument principle, i.e. the integral 
$\frac{1}{2\pi\sqrt{-1}}\int_{\partial \Delta}\frac{F'(y_0,z)}{F(y_0,z)}dz$ 
equals the number of zeroes. Since it is a continuous (holomorphic?) function on
$\Delta_y$ and integer valued, it must be constant.

The condition that $\lim_{z_n\to0}\frac{F(0,z_n)}{z_n^k}\neq 0,\infty$ means
that the fiber at 0 has a zero of order $k$:
$$
\frac{F(0,z_n)}{z_n^k}=\frac{F(0)}{z_n^k}+F'(0)\frac{z_n}{z_n^k}+\ldots
+\frac{F^{(k)}(0)}{k!}\frac{z_n^k}{z_n^k}+\ldots
$$
Thus, on other fibers we can have
more zeroes, but without multiplicities they are always $k$.
\item 
\end{enumerate}
\end{proof}

\begin{definition}
\label{definition-Weierstrass-polynomial}
A {\it Weierstrass polynomial} is a function $f \in \mathcal{O}_{n-1}[z_n]$,
that is, a function which is polynomial in the last variables with coefficients
that are analytic functions on the first $n-1$ variables.
\end{definition}

\begin{exercise}
\label{exercise-Weierstrass-preparation-theorem}
Let $F$ be an analytic function in a neighbourhood of $0$ in $\mathbb{C}^n$,
such that $\lim_{z_n\to 0}\neq 0,\infty$. Consider the projection map
$\Pi:\mathbb{C}^n\to\mathbb{C}^{n-1}$, 
$(z_1,\ldots,z_n)\mapsto(z_1,\ldots,z_{n-1})$, and let 
$P(z_n)\in \mathcal{O}_{n_-1}[z_n]$ be the Weierstrass polynomial given by 
$P(z_n)=\sum_{i=0}^ke_iz_n^i$, where $e_i$ are the elementary symmetric
polynomials on the zeros $\alpha_1,\ldots,\alpha_k$ defined in the previous
exercise. Prove that $F=P(z_n)u$, where $u$ is a germ of an invertible
holomorphic function.
\end{exercise}

\begin{proof}
For every $y_0$ we can write
$$
F(y_0,z_n)=(z_n-\alpha_1)\ldots(z_n-\alpha_n)h(z_n)
$$
where implicitly all $\alpha_i$ and $h$ depend on $y_0$. The product of 
$(z_n-\alpha_i)$ is the definition of $P$. Varying $y_0$ we obtain the result.
\end{proof}

This argument works for every function that has a zero of order $k$ in
$0\in\mathbb{C}^n$.

\begin{theorem}[Weierstrass preparation theorem]
\label{theorem-Weierstrass-preparation}
If $f:U\subset\mathbb{C}^n\to\mathbb{C}$ is holomorphic and $f$ is not
identically zero in the coordinate axis $z_n:=w$, there
is a unique germ of a monic Weierstrass polynomial $g$ whose coefficients are
holomorphic functions on the first $n-1$ variables 
and a germ of a holomorphic 
function $h$ with $h(0)\neq 0$ such that $f=gh$.
\end{theorem}

\begin{proof}
Since $f$ is not identically zero near $0$, then there is a point that we may
suppose is in the $z_n$-axis where $f$ is not zero.

Consider the function of one complex variable $f(0,\ldots,0,w)$. Since it has a
zero at $0$ (why?), is holomorphic, and is not identically zero, the zero $0$
must be of order $k$. We have shown that there is a choice of $r$ and  $r'$ such
that for every $y_0 \in B_r(z_1,\ldots,z_{n-1})\times \Delta_{r'}(z_n)$ the
number of zeroes of $F(y_0,z_n)$ is $k$.

We also showed that the Newton polynomials $p_d=\sum_{i=1}^k\alpha_i^d$ are
holomorphic functions of $y_0$. We also showed that the elementary symmetric
polynomials can be expressed in terms of $p_i$, so that the former are also
holomorphic functions of $y_0$.

Then we define for every $y_0$ the elementary symmetric polynomial on
$\alpha_i$, $P(z_n)=\prod_{i=1}^k(z_n-\alpha_i)$ where $\alpha_i$ are the roots 
of $F(y_0,z_n)$. Since the elementary symmetric polynomials are holomorphic
functions, it follows that $P$ is a Weierstrass polynomial. Varying $y_0$ we
obtain the desired expression. 
\end{proof}

\begin{lemma}
\label{lemma-Gauss-UFD}
If $R$ is a UDF, then $R[x]$ is a UFD.
\end{lemma}

\begin{lemma}
\label{lemma-stalk-is-UFD}
The stalk $\mathcal{O}_n:=\mathcal{O}_{\mathbb{C}^n,0}$ is a UFD.
\end{lemma}

\begin{proof}
By induction on $n$. For $n=0$ it is trivial. Suppose $\mathcal{O}_{n-1}$ is a
UFD. Then by Gauss' Lemma \ref{lemma-Gauss}, $\mathcal{O}_{n-1}[w]$ is a UFD
too. Thus we may express any Weierstrass polynomial $g$ as a product of
irreducible elements (uniquely up to multiplication by units).

Let $f\in \mathcal{O}_n$. We want to express $f$ as a product of (unique up to
multiplication by units) of irreducible elements. By Weierstrass Preparation
Theorem \ref{theorem-Weierstrass-preparation} there is a Weierstrass polynomial
$g\in\mathcal{O}_n[w]$ and a holomorphic function not vanishing on $0$ (i.e. a
unit of $\mathcal{O}_n$) such that $f=gh$. By the previous remark $g$ is
factored uniquely up to multiplication by units as $g=g_1\ldots g_m$. This shows
existence of the factorization.

To prove uniqueness suppose that $f=f_1\ldots f_k$ for some irreducible
$f_1,\ldots,f_k\in\mathcal{O}_n$. Since $f$ does not vanish in the $w$ axis,
neither can each $f_i$, so that we may decompose each of them as  $f_i=g_i'h_i$
by Weierstrass Preparation Theorem. Since $f_i$ is irreducible, it follows that
$g_i'$ is irreducible. Then we have that $$ f=gh=\prod g_i'\prod h_i $$ so by
uniqueness in Weierstrass Preparation Theorem we conclude that $g=\prod g_i'$,
and by uniqueness from the fact that  $\mathcal{O}_n[w]$ is a UFD we conclude
that $g$ coincides with $\prod g_i'$ up to multiplication by units.
\end{proof}

\bibliography{my}
\bibliographystyle{amsalpha}

\end{document}
