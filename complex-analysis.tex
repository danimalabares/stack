\input{preamble}

\begin{document}

\title{Complex Analysis}
\maketitle

\phantomsection
\label{section-phantom}
\hfill
\href{http://github.com/danimalabares/stack}{github.com/danimalabares/stack}

\tableofcontents

\section{Holomorphic functions}
\label{section-holomorphic-functions}

\begin{definition}
\label{definition-holomorphic-function}
A function $f:W\subset \mathbb{C} \to \mathbb{C}$ is {\it holomorphic} at $z_0
\in U$ if
$$
\lim_{z\to z_0} \frac{f(z_0-z)-f(z_0)}{z-z_0}
$$
exists. This is equivalent to
$$
\lim_{h\to 0} \frac{f(z_0+h)-f(z_0)}{h}
$$
where $h$ is a complex parameter.
\end{definition}

\section{Cauchy-Riemann equations}
\label{section-Cauchy-Riemann-equations}

\begin{theorem}
\label{theorem-Cauchy-Riemann}
Let $f:W\subset\mathbb{C}\to \mathbb{C}$ be a function. Let $z=x+iy$ coordinates
in $\mathbb{C}$. Define $f:=u+iv$ and
$z_0=x_0+iy_0 \in U$. $f$ is holomorphic at $z_0$ if and only if
$$
\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y},\qquad \frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}
$$
\end{theorem}

\section{Integration}
\label{section-integration}

\noindent
The upshot about complex integration
(and complex analysis, really)
is that differential forms $fdz$ with $f$ holomorphic are exact
because that's equivalent to Cauchy-Riemann equations.

\medskip\noindent
The complex integral may first be defined for a complex-valued function $f$
defined on a real integral as
$$
\int_a^bf=\int_a^b u + i \int_a^bv
$$
And then for curves as
$$
\int_\gamma f:=\int_a^b f(\gamma(t))\gamma'(t)dt
$$
A natural question is: what's the essential difference between complex and real
analysis? What is the power behind Cauchy-Riemann equations? The answer is in
the proof of Cauchy integral theorem \ref{theorem-Cauchy}: Stokes theorem, which
in dimension 2 is called Green's theorem, gives an integrand with Cauchy-Riemann
equations.

Recall
\begin{theorem}[Stokes]
\label{theorem-Stokes}
\begin{reference}
\cite[Chapter 4, Theorem 1]{ahl}
\end{reference}
The line integral $\int_\gamma p dx +q dy$ defined in $\Omega$ depends on the
end points of $\gamma$ if and only if there exists a function $U(x,y)$ in
$\Omega$ with the partial derivatives $\partial U/\partial x=p$ and 
$\partial U/\partial y=q$, that is, if $pdx +qdy$ is exact.
\end{theorem}

\begin{proof}
Fundamental theorem of calculus.
\end{proof}

\noindent
So, when computing a complex integral $\int_\gamma f$, we are interested in
knowing when is the form $fdz$ exact. Which says that there is a function $F$
such that $\partial_x F=f$ and $\partial_y F=if$. In fact, this is the same as
asking that $F$ satisfies the Cauchy-Riemann equations (provided $f$ is
continuous).

Thus, the integral $\int_\gamma f$ for continuous $f$ depends only on the end
points of $\gamma$ if and only if $f$ is the derivative of an analytic function
in $\Omega$.

\medskip\noindent
The theory of complex integration has to do with homology. The following
result does not hold for domains that are not simply connected:

\begin{theorem}[Cauchy]
\label{theorem-Cauchy}
If $f(z)$ is analytic in an open disk $\Delta$, then
\begin{equation}
\label{equation-Cauchy-theorem}
\int_\gamma f(z)dz=0
\end{equation}
for every closed curve $\gamma$ in $\Delta$.
\end{theorem}

\begin{proof}
Recall that Green's theorem says that
$$
\int_{\partial\Delta}(vdx+udy)=\int_\Delta d(vdx+udy)
=\int_\Delta(\partial_xu-\partial_yv)\text{Vol}
$$
There's Cauchy-Riemann equations!
\end{proof}

\begin{proof}[Alternative idea]
I prove this using Stokes theorem. The fact that $f$ is holomorphic says that
$\overline{\partial}f=0$. In turn, $d=\partial+\overline{\partial}$, so that
$df=\partial f$, which is a $(1,0)$-form (just as $\overline{\partial}$ maps
functions to $(0,1)$-forms… (must justify…)). This makes $d(fdz)=df\wedge dz$ a
$(2,0)$-form, but there are no such forms on $\mathbb{C}$ (because there are no
holomorphic or antiholomorphic 2-forms on a complex dimension 1 space).
\end{proof}

\noindent
The following limit condition allows for the former result even if the domain
has some points removed:

\begin{theorem}
\label{theorem-Cauchy-removable}
Let $f$ be analytic in the region $\Delta'$ by omitting a finite number of
points $\zeta_j$ from an open disk $\Delta$. If $f$ satisfies that
\begin{equation}
\label{equation-removable-singularity}
\lim_{z\to\zeta_j} (z-\zeta_j)f(z)=0
\end{equation}
for all $j$, then Eq. \ref{equation-Cauchy-theorem} holds for any closed curve
$\gamma$ in $\Delta'$.
\end{theorem}

\begin{proof}[Sketch of proof]
It suffices to prove that Eq. \ref{equation-removable-singularity} implies that
we can extend $f$ to a holomorphic function on all of $\Delta$ and apply Stokes
as in Theorem \ref{theorem-Cauchy}. But that won't work: for that we need Cauchy
integral formula, and that's why I'm here: to prove Cauchy integral formula.

So I think it might be using logarithmic derivative. The limit above allows to
write
$$
|f(z)|\leq \frac{\varepsilon}{|z-\zeta_j|}
$$
Then integrate. On the left we can bound the integral of $f$ after applying
integral triangle inequality, and on the right we have logarithmic derivative of
$z-\zeta_j$. This will be a fixed number (the index, see Definition
\ref{definition-index}), so that we have effectively bounded the integral.
\end{proof}

\section{Cauchy integral formula}
\label{section-Cauchy-integral-formula}

\noindent
The way to Cauchy's integral formula is basically:
holomorphic functions satisfy Cauchy-Riemann equations,
which give Cauchy's integral theorem 
(which is just a consequence Stokes)
and then apply this to the function $\frac{f(z)-f(z_0)}{z-z_0}$.

The theorem says that we can compute the value of
a holomorphic function at a point as an integral around the point.

To prove a general version we shall use the notion of index of a curve
about a point, which tells us how many times the curve winds about the point.

\begin{lemma}
\label{lemma-index-is-multiple-of-2pii}
\begin{reference}
\cite[Section 2.2, Lemma 1]{ahl}
\end{reference}
If the piecewise differentiable closed curve $\gamma$ does not pass through the
point $a$, then the value of the integral
$$
\int_\gamma\frac{dz}{z-a}
$$
is a multple of $2\pi i$.
\end{lemma}

\begin{proof}[Non-proof]
We are tempted to simply write the integrand as the logarithmic derivative of
the function $f(z)=z-a$. But this isn't quite right, we must be careful with the
domain of the logarithm. But it is instructive to see the computation:
$$
\int_\gamma\frac{dz}{z-a}=\int_\gamma d \text{log}(z-a)=
\int_\gamma d\text{log}|z-a|+i\int_\gamma d \text{arg}(z-a)
$$
If  $\gamma$ is closed then $\text{log}|z-a|$ would return to its initial value
and $\text{arg}(z-a)$ increases or decreases by a multiple of $2\pi$. The actual
proof by Ahlfors is different.

Still, the proof for a circle about a point $z_0$ is also trivial: this is just
the case of the integral $\int_{\partial\Delta}\frac{dz}{z-z_0}$, but the curve
here is  $\gamma(t)=e^{it}+z_0$ so that when substituting in the parametrization
we obtain $\int_0^{2\pi}\frac{ie^{it}}{(e^{it}+z_0)-z_0}=2\pi i$.
\end{proof}

\begin{definition}
\label{definition-index}
The {\it index} of the point $a$ with respect to the closed curve $\gamma$ is
\begin{equation}
\label{equation-index}
n(\gamma,a):=\frac{1}{2\pi i}\int_\gamma\frac{dz}{z-a}
\end{equation}
\end{definition}

\begin{theorem}[Cauchy Integral Formula]
\label{theorem-Cauchy-integral-formula}
\begin{reference}
\cite[Section 2.2, Theorem 6]{ahl}
\end{reference}
Suppose that $f(z)$ is analytic in an open disk $\Delta$, and let $\gamma$ be a
closed curve in $\Delta$. For any point not on $\gamma$,
\begin{equation}
\label{equation-Cauchy-formula-with-index}
n(\gamma,a)f(a)=\frac{1}{2\pi i}\int_\gamma\frac{f(z)dz}{z-a}
\end{equation}
where $n(\gamma,a)$ is the index of $a$ with respect to $\gamma$.
\end{theorem}

\begin{proof}
This is a simple application of Theorem \ref{theorem-Cauchy-removable} for the
function
$$
F(z):=\frac{f(z)-f(a)}{z-a}
$$
Notice the limit condition holds, so that the integral vanishes!
\end{proof}

\noindent
Notice that we can differentiate Cauchy integral formula to obtain
\begin{equation}
\label{equation-first-derivative-Cauchy-formula}
f'(z)=\frac{1}{2\pi i}\int_\gamma\frac{f(\zeta)}{(\zeta-z)^2}d\zeta
\end{equation}
and more generally
$$
f^{(n)}(z)=\frac{n!}{2\pi i}\int_\gamma \frac{f(\zeta)}{(\zeta-z)^{n+1}}
$$
Though we can just say that integrals can be differentiating by splitting into
real and imaginary part, and using the real derivative, a technical lemma is 
used by Ahlfors to confirm that indeed we can differentiate
under the integral sign.

\begin{lemma}
\label{lemma-technical-lemma}
[missing]
\end{lemma}

\noindent
The importance of this result is that it shows that a holomorphic function has
derivatives of all degrees.

Then we state two interesting results:

\begin{theorem}[Morera]
\label{theorem-Morera}
\begin{reference}
\cite[p. 122]{ahl}
\end{reference}
If $f(z)$ is defined and continuous in a region $\Omega$, and if 
$\int_\gamma fdz=0$ for all closed curves $\gamma$ in $\Omega$, then $f(z)$ is
analytic in $\Omega$.
\end{theorem}

\noindent
This is what Lee calls a {\it conservative covector field}, i.e. a form whose
integral on closed curves vanishes. The proof of Morera's theorem then reduces
to the fact that a covector field is conservative if and only if it is exact
\cite[Theorem 11.42]{les}. The reverse implication is the fundamental theorem of
calculus. The forward implication is not very straightforward in \cite{les}.

As a final remark, I add that (\cite[Propoistion 11.40]{les}) a smooth covector
field is conservative if and only if its line integrals are path-independent, in
the sense that integrals coincide along two {\it piecewise} smooth curve
segments with the same starting and ending points. That is, form is exact
implies integral independent of homotopy class?

We finish with

\begin{theorem}[Liouville]
\label{theorem-Liouville}
A bounded holomorphic function defined on all of $\mathbb{C}$ must be constant.
\end{theorem}

\begin{proof}
Let $\gamma$ be a circle of radius $r$ about $z$, then by the first derivative
of Cauchy formula (Eq. \ref{equation-first-derivative-Cauchy-formula}),
$$
|f'(z)|\leq \frac{M}{2\pi i}\int_\gamma \frac{d\zeta}{|\zeta-z|^2}
$$
and compute the integral, of course is $\frac{2\pi i}{r}$.
\end{proof}

\noindent
A fun application of this is proving the fundamental theorem of algebra. If a
complex polynomial had no zeroes, $1/P(z)$ would be analytic in all of
$\mathbb{C}$, and since $P(z)$ tends to $\infty$ as $z$ tends to $\infty$,
$1/P(z)$ is bounded (use Riemann sphere argument).

\section{Zeroes of a holomorphic function}
\label{section-zeroes}

\noindent
Perhaps the best way to remember this section is by the formula
\begin{equation}
\label{equation-zero}
f(z)=f_n(z)(z-z_0)^n
\end{equation}
which holds for a holomorphic function which
vanishes on $z_0$ and whose derivatives $f^{(\nu)}(z_0)$ vanish
for $\nu<n$ (see \cite[p. 126]{Ahlfors}).
Here $f_n$ is a holomorphic function which does not vanish at $z_0$ 
(according to \cite{lec}, but why?).
This fact shows that the zero $z_0$ is isolated,
since $f_n(z)$ is continuous and nonzero in $z_0$,
and $(z-z_0)^n$ is also nonzero in a pointed neighbourhood of $z_0$.

All this is basically due to Cauchy integral formula,
as the following theorem shows.

\begin{theorem}[Removable singularity theorem]
\label{theorem-removable-singularity}
Let $f(z)$ be analytic in a region $\Omega'$ obtained by omitting a point $z_0$
from a region $\Omega$. There exists an analytic function  defined on all 
$\Omega$ that coincides with $f$ on $\Omega'$ if and only if 
$\lim_{z \to z_0} (z-z_0)f(z)=0$. Such a function is unique.
\end{theorem}

\begin{proof}
(What is wrong in this proof? I need to use
the limit condition somewhere… see 
\href{https://en.wikipedia.org/wiki/Removable_singularity}{wiki} and
\href{https://en.wikipedia.org/wiki/Analyticity_of_holomorphic_functions}{wiki}
)
The proof is just looking at Cauchy integral formula and noticing that the
function defined at the singularity via the integral around it is holomorphic
since it is given by an integral.
Integrals are holomorphic since partial derivatives, which are limits,
go through the integral sign, meaning we can differentiate
infinitely many times and obtain a Taylor series expansion.
(or perhaps more generally by Technical
Lemma \ref{lemma-technical-lemma}).
\end{proof}

\noindent
Now consider the function
\begin{equation}
\label{equation-F}
F(z)=\frac{f(z)-f(z_0)}{z-z_0},
\end{equation}
which has $\lim_{z\to z_0} (z-z_0)F(z)=0$. Then we apply the theorem and find
that the holomorphic function extending $F$, say $f_1$, is actually $f'$ at
$z_0$ by definition. 
Substituting $F$ by $f_1$
in Equation \ref{equation-F} we obtain
$f(z)=f(z_0)+f_1(z)(z-z_0)$. Applying the same
argument to $f_1$ yields $f_1(z)=f_1(z_0)+f_2(z)(z-z_0)$ for some function
$f_2$ that is $f''$ at $z_0$.

Repeating this process yields:

\begin{lemma}[Finite Taylor expansion]
\label{lemma-finite-Taylor-expansion}
\begin{reference}
\cite[Chapter 4, Section 3, Subsection 2, Theorem 8]{ahl}
\end{reference}
If $f$ is analytic in a connected open set $W\subset\mathbb{C}$ and $a \in W$,
then we can write
\begin{equation}
\label{equation-finite-Taylor-expansion}
f(z)=f(a)+f'(a)(z-a)+\frac{f''(a)}{2!}(z-a)^2+\ldots
+\frac{f^{(n-1)}(a)}{(n-1)!}(z-a)^{n-1}+f_n(z)(z-a)^n
\end{equation}
for some function $f_n$ analytic on $W$.
\end{lemma}

\begin{theorem}
\label{theorem-zeroes-are-isolated-and-have-finite-order}
\begin{reference}
\cite[p. 13]{lec}
\end{reference}
If $f:W\subset\mathbb{C}\to \mathbb{C}$ is a holomorphic function defined on an
open set $W$ and $f(a)=0$ for some $a\in W$, and $f$ is not identically zero,
then there is a disk $D_r(a)\subseteq W$ such that $f(z)\neq 0$ for $z \in
D_r(a)\setminus\{a\}$ and a positive integer $m$ called the {\it order} or
{\it multiplicity} of the zero $a$ such that  $f(z)=(z-a)^mh(z)$ for some
holomorphic function $h$ that does not vanish at $a$. The order of a zero is
equal to the smallest integer $m$ such that $f^{(m)}(a)\neq 0$.
\end{theorem}

\begin{proof}
If $f$ is not identically zero, there must exist a first derivative $f^{
(h)}(a)$ that is not zero, since otherwise $f$ would vanish identically in $W$.
This follows from … 

Then by Lemma \ref{lemma-finite-Taylor-expansion} we obtain that
$f(z)=f_h(z-a)^n$.
\end{proof}

\section{Taylor expansion}
\label{section-Taylor-expansion}

\noindent
Actually this part is quite later in Ahlfors book because it requires
the theory of convergence of series.
To pass from the finite Taylor expansion to the
infinite version we notice that the 
term $f_{n+1}$ in Equation \ref{equation-finite-Taylor-expansion}
is of the form
$$
f_{n+1}(z)=\frac{1}{2\pi i}\int_\gamma \frac{f(\zeta)d\zeta}
{(\zeta-z_0)^{n+1}(\zeta-z)}
$$
where $\gamma$ is a curve of radius $\rho$.
Then ``we obtain at once''
$$
|f_{n+1}(z)(z-z_0)^{n+1}|\leq \frac{M|z-z_0|^{n+1}}{\rho^n(\rho-|z-z_0|}.
$$
Then we must observe that the bound converges uniformly
to zero in every disk of radius smaller than $\rho$,
and by Weierstrass theorem there should be a limit
function identical to zero.

\section{Argument Principle}
\label{section-argument-principle}

\noindent
A simple version of the Argument Principle does not need Residue theorem:

\begin{exercise}
\label{exercise-argument-principle}
Let $f$ be a holomorphic function on a disk, non-zero on $\partial \Delta$, and
let $S_k(f):=\frac{1}{2\pi\sqrt{-1}}\int_{\partial\Delta}\frac{f'}{f}z^kdz$.
Prove the $S_k(f)=\sum d_i\alpha_i^k$ where $\alpha_i$ are all zeroes of $f$ and
$d_i$ their multiplicities.
\end{exercise}

\begin{proof}
We use Lemma \ref{lemma-finite-Taylor-expansion} to write
$f(z)=(z-z_1)h_1(z)$ where $z_1$ is a zero of $f$. 
Applying that to $h_1$ for another zero  $z_2$ of $f$,
and continuing in this way, we obtain 
\begin{equation}
\label{equation-factor-zeroes}
f(z)=(z-z_1)(z-z_2)\ldots(z-z_n)h(z).
\end{equation}

\noindent
Computing the quotient we obtain
$$
\frac{f'(z)}{f(z)}=\frac{\text{derivative of
$(z-z_1)\ldots(z-z_n)$}}{(z-z_1)\ldots(z-z_n)}+\frac{h'(z)}{h(z)}
$$
The right hand term will vanish upon integration since it is a holomorphic
function (with no poles because $h(z)\neq 0$). The left-hand term will give a
sum of $\frac{1}{z-z_i}$ upon differentiation. As it is, the result of
integration is the sum of the orders of each zero i.e. the number of zeroes
counted without multiplicity 
(because the integrals are all 1, and there's one for each zero without
multiplicity).

Multiplying by $z^k$ yields the desired result by Cauchy formula
\ref{equation-Cauchy-formula-with-index}. More exactly,
multiplying $z^k$ in the formula gives instead of a sum
of integrals $\int \frac{1}{z-z_i}$, a sum of integrals
$\int\frac{z^k}{z-z_i}$ which by Cauchy formula give $z_i^k$
(disregarding the factor $2\pi\sqrt{-1}$).
\end{proof}

\begin{theorem}[Argument Principle]
\label{theorem-argument-principle-and-Rouche-theorem}
\begin{reference}
\cite[Chapter 5, Theorem 18]{ahl}
\end{reference}
If $f$ is meromorphic in $\Omega$ with zeros $a_j$ and poles $b_k$, then
\begin{equation}
\label{equation-argument-principle}
\frac{1}{2\pi i}\int_\gamma\frac{f'(\zeta)}{f(\zeta)}d\zeta
=\sum_{i}n(\gamma,a_i)-\sum_{k}n(\gamma,b_k)
\end{equation}
\end{theorem}

\begin{lemma}[Rouché theorem]
\label{lemma-Rouche-theorem}
\begin{reference}
\cite[Chapter 5, Corollary, p. 153]{ahl}
\end{reference}
Let $\gamma$ be homologous to zero in $\Omega$ and such that $n(\gamma,z)$ is
either 0 or 1 for any point $z$ not on $\gamma$. Suppose that $f$ and $g$ are
analytic in $\Omega$ and satisfy that $|f-g|<|f|$ on $\gamma$. Then $f$ and $g$
have the same number of zeros enclosed by $\gamma$.
\end{lemma}

Compare with Misha's version

\begin{theorem}[Rouché theorem]
\label{theorem-Rouche-theorem-Mishas-version}
Let $f_t$ be a family of holomorphic functions on a disk  $\Delta$, continuously
depending on a parameter $t\in \mathbb{R}$ and non-zero everywhere on its
boundary $\partial\Delta$. Prove that the number of zeros of $f_t$ in $\Delta$
is constant.
\end{theorem}

\begin{proof}
Consider the map $t\mapsto f_t\mapsto S(f_t)$, which is continuous by hypothesis
and because $S$, being an integral, is continuous. Then we obtain a continuous
map $\mathbb{R}\to\mathbb{R}$ with integer values, meaning it must be constant.
\end{proof}

A similar argument may be used to prove that a holomorphic function $F$ defined
on $\Delta\times\Delta$ gives a holomorphic map 
\begin{equation}
\label{equation-zeros-on-polydisk}
y_0\mapsto \int_{\partial\Delta}\frac{F'(x,y_0)}{F(x,y_0)}\phi(x)dx
=\sum_id_i\phi(\alpha_i)
\end{equation}
for any holomorphic function $\phi:\Delta\to\mathbb{C}$.

\section{Holomorphic functions in several variables}
\label{section-holomorphic-functions-in-several-variables}

\begin{lemma}
\label{lemma-holomorphic-function-characterization}
\cite{lec}, Theorem 1.21. Let $U\subseteq\mathbb{C}^n$ be open and $f:U\to
\mathbb{C}$. The following are equivalent:
\begin{enumerate}
\item $f$ is holomorphic (i.e. it is continuous and has a complex partial
derivative with respect to each variable at each point of $U$)
\item $f$ is smooth and satisfies the following Cauchy-Riemann equations:
\begin{equation}
\label{equation-Cauchy-Riemann-several-variables}
\frac{\partial u}{\partial x^j}=\frac{\partial v}{\partial y^j},\qquad 
\frac{\partial u}{\partial y^j}=-\frac{\partial v}{\partial x^j}
\end{equation}
where $z^j=x^j+\sqrt{-1}y^j$ and $f(s)=u(z)+\sqrt{-1}v(x)$.
\item For each $p=(p^1,\ldots,p^n)\in U$ there exists a neighbourhood of $p$ in
$U$ on which $f$ is equal to the sum of an absolutely convergent power series of
the form
\begin{equation}
\label{equation-Taylor-series-several-variables}
f(z)=\sum_{i_1,\ldots,i_n}a_{i_1,\ldots,i_n}(z^1-p^1)\ldots(z^n-p^n)
\end{equation}
\end{enumerate}
\end{lemma}

\begin{proof}
I will prove that if $f$ is holomorphic then it has a Taylor series for $n=2$. 
First apply Cauchy integral formula on each variable to obtain
$$
f(z^1,z^2)=\frac{1}{(2\pi\sqrt{-1})^2}
\int_{\substack{|z^1-w^1|=r \\ |z^2-w^2|=r}}
\frac{f(w^1,w^2)}{(w^1-z^1)(w^2-z^2)}dw^1dw^2
$$
Now observe:
\begin{equation}
\label{equation-multivariable-Cauchy}
\frac{1}{w^1-z^1}=\frac{1}{w^1-p^1+p^1-z^1}
=\frac{1}{w^1-p^1}\frac{1}{1-\frac{p^1-z^1}{w^1-p^1}}
\end{equation}
And on the right-hand-side we have a geometric series so that we may write
$$
\frac{1}{w^1-z^1}
=\frac{1}{w^1-p^1}\sum_{k=0}^\infty\left(\frac{p^1-z^1}{w^1-p^1}\right)^k
$$
finally substituting this into (\ref{equation-multivariable-Cauchy}) we may take
the products $(p^1-z^1)^{k_1}(p^2-z^2)^{k_2}$ out of the integral and define the
remaining term as $a_{k_1k_2}$.
\end{proof}

\section{Taylor series in several variables}
\label{section-Taylor-series-in-several-variables}

\section{Identity theorem in several variables}
\label{section-identity-theorem-in-several-variables}

\begin{theorem}[Identity theorem]
\label{theorem-identity-several-variables}
\begin{reference}
\cite[Proposition 1.28]{lec}
\end{reference}
If two holomorphic functions $f,g:W\subset \mathbb{C}^n \to \mathbb{C}$ coincide
in an open subset of the connected open set $W$, 
then they coincide in all of $W$.
\end{theorem}

\begin{proof}
Let $U$ be the set where the function $h:=f-g$ and all its partial derivatives
vanish. Then $U$ is open since for every point in $U$ there is a neighbourhood
where $h$ is expressed as a Taylor series, whose coefficients must be given by
the partial derivatives of $h$. By definition of $U$, we see that $h$ must be
zero in such a neighbourhood. $U$ is also closed by continuity of partial
derivatives of all orders. 
\end{proof}

\section{Germs of holomorphic functions}
\label{section-germs-of-holomorphic-functions}

\begin{definition}
\label{definition-germ-of-holomorphic-function}
Let $U,U' \subset \mathbb{C}^n$ be neighbourhoods of $0$ and $f \in
\mathcal{O}_U$, $f'\in\mathcal{O}_{U'}$ holomorphic functions. We say that $f$
and $f'$ have the same germ, $f\sim f'$, if $f|_{U\cap U'}=f'|_{U\cap U'}$.
Clearly (?), $\sim$ gives an equivalence relation on the set of pairs $(U\ni 0,
f\in \mathcal{O}_U)$. An equivalence class is called {\bf germ of a holomorphic
function}. The space of germs in $0$ of holomorphic functions on $\mathbb{C}^n$
is denoted $\mathcal{O}_n$.
\end{definition}

\begin{exercise}
\label{exercise-stalk-is-not-finitely-generated-over-C}
Prova that the ring $\mathcal{O}_n$ of germs of holomorphic functions is not
finitely generated over $\mathbb{C}$ for any $n>0$.
\end{exercise}

\begin{proof}
I think the existence of $e^z$ as a holomorphic function satisfying the
differential equation $f'=f$ is enough to show that the coefficients of its
Taylor polynomial are all nonzero. This argument works for several variables as
well.
\end{proof}

\begin{definition}
\label{definition-formal-power-series}
A {\it formal power series} in the variables $t_1,\ldots,t_n$ is a sum
$\sum_{i=0}^{\infty}P_i(t_1,\ldots,t_n)$ where $P_i$ are homogeneous polynomials
of degree $i$. Addition of power series is defined componentwise, and
multiplication is defined via
$$
\left(\sum_{i=0}^\infty P_i(t_1,\ldots,t_n)\right)
\left(\sum_{i=0}^\infty Q_i(t_1,\ldots,t_n)\right)
=\sum_{i=0}^\infty R_i(t_1,\ldots,t_n)
$$
where $R_d(t_1,\ldots,t_n)=\sum_{i+j=d}P_i(t_1,\ldots,t_n)Q_j(t_1,\ldots,t_n)$.
\end{definition}

We can think of germs of functions in $\mathcal{O}_n$ as elements in the ring of
power series $\mathbb{C}[\![t_1,\ldots,t_n]\!]$. I think there is no problem to
prove this statement, nor the fact that power series is actually a ring with
units the nonzero constants and zero the zero constant.

\begin{exercise}
\label{exercise-stalk-has-no-zero-divisors}
Prove that $\mathcal{O}_n$ has no zero divisors.
\end{exercise}

\begin{proof}
Suppose that $PQ=0$ but neither of $P$ nor $Q$ are zero. Then $P_i \neq 0$ for
some $i$ and $Q_j \neq 0$ for some $j$. Then we can write
$$
P_iQ_j=-\sum_{\substack{p+q=i+j \\ p\neq i,q\neq j}}P_pQ_q
$$
And then what. Other way is by induction. For $n=0$ we are the complex numbers
so no problem. Suppose that $\mathcal{O}_n$ has no zero divisors. Then it looks
like we can deal with degrees smaller than $n+1$, but the $d$-th term is not a
simple product of $P_iQ_j$ with $i+j=d$, but a sum. So not sure too.
\end{proof}

\begin{definition}
\label{definition-local-ring}
A ring $R$ is called {\it local} if it contains an ideal $I\subset R$ such that
all elements $r \not \in I$ are invertible.
\end{definition}

It is an easy exercise to show that this definition is equivalent to having a
unique maximal ideal.

\begin{exercise}
\label{exercise-power-series-is-not-finitely-generated-over-stalk}
Prove that the ring $\mathbb{C}[\![ t_1,\ldots,t_n]\!]$ is not
finitely generated over $\mathcal{O}_n \subset 
\mathbb{C}[\![ t_1,\ldots,t_n]\!]$.
\end{exercise}

\begin{proof}
I thought that $\mathcal{O}_n$ would be the same as 
$\mathbb{C}[\![ t_1,..,t_n]\!]$… the natural map is surely injective
but why not surjective? There are power series that are not holomorphic
functions? Maybe because of radius of convergence? No, because germs of
holomorphic functions can be defined very near the origin… Every power series
has a nonzero radius of convergence, right?
\end{proof}

\noindent
Now I will discuss zeroes of holomorphic functions of several variables.

\begin{definition}
\label{definition-zero-of-holomorphic-function-of-several-variables}
Let  $f\in \mathcal{O}_n$ be a germ of holomorphic function on $\mathbb{C}^n$.
Write its Taylor series $f(z)=\sum_{i=0}^\infty P_i(t_1,\ldots,t_n)$, where
$P_i$ are homogeneous polynomials of degree $i$. We say that $f$ has a {\it
zero of order (or multiplicity) $k$ in $0$} if $P_0=\ldots=P_{k-1}=0$. In this
situation {\it principal part} of the function $f$ is the homogeneous polynomial
$P_k$.
\end{definition}

The following exercise shows that a holomorphic change of coordinates will
preserve the order of a zero, and the principal part of the new function will be
determined by the differential of the change of coordinate map. We will need to
change coordinates when we do Weierstrass Preparation theorem
\ref{theorem-weierstrass-preparation}.

\begin{exercise}
\label{exercise-principal-part-under-coordinate-change}
Let $\Phi(t_1,\ldots,t_n)=(F_1(t_1,\ldots,t_n),\ldots,F_n(t_1,\ldots,t_n))$ be
the holomorphic coordinate change around zero (?), with $F_i(0,\ldots,0)=0$ and
$A:=\left(\frac{\partial F_i}{\partial t_j}\right)_0$ its differential (I suppose
$\det A \neq 0$. Prove that
\begin{enumerate}
\item For any germ $f\in \mathcal{O}_n$ which has multiplicity $k$, the function
$\Phi^*(f)$ has zero of the same multiplicity.
\item The principal part of $\Phi^*(f)$ is obtained from the principal part of
$f$ by action of $A$.
\end{enumerate}
\end{exercise}

\begin{proof}[Sketch of proof]
\begin{enumerate}
\item The condition that $\det A \neq 0$ implies that all $F_i$ must have linear
term---otherwise their partial derivatives would vanish when evaluated at zero.
When substituting $f(z_1,\ldots,z_n)=\sum_{|\alpha|\geq k}\alpha z^\alpha$ with
$\Phi$ we find that there must be a term of order $k$.
\item The case for $n=1$ is clear. The general case follows, I think, from the
observation that the derivative $A$ recovers the linear terms, which, as shown
in the previous item, correspond to the principal part of $\Phi^*f$.
\end{enumerate}
\end{proof}

\begin{exercise}
\label{exercise-zero-locus-of-homogeneous-polynomial}
Let $Q$ be a non-zero homogeneous polynomial on $t_0,\ldots,t_n$, and $V(Q)$ its
zero set, which we consider as a subset in $\mathbb{C}P^{n}$.
\begin{enumerate}
\item Prove that $\mathbb{C}P^{n}\setminus V(Q)$ is non-empty.
\item Prove that $V(Q)\subset\mathbb{C}P^{n}$ is a set of measure 0.
\end{enumerate}
\end{exercise}

\begin{proof}
\begin{enumerate}
\item This follows from Identity Theorem
\ref{theorem-identity-several-variables}. Indeed, if $V(Q)$ was all of
$\mathbb{C}^{n}$, it would vanish on an open set, implying that $Q$ is
identically zero.
\item $V(Q)$ may be decomposed in the sets of regular and singular points.
Regular points have submanifold charts, while singular points have measure zero
by Sard's theorem.
\end{enumerate}
\end{proof}

\begin{exercise}
\label{exercise-principal-parts}
Let $f_1,f_2,\ldots\in \mathcal{O}_n$ be a countable collection of germs, which
vanish with multiplicity $k_1,k_2,\ldots$. Prove that there exists a coordinate
system $z_1,\ldots,z_n$ such that 
$\lim_{z_n\to 0} \frac{f_i(0,z_n)}{z_n^{k_i}}\neq 0$ for all $i$.
\end{exercise}

\begin{proof}[Sketch of proof]
I don't understand this exercise: evaluating any germ $f_i$ on $(0,z_n)$ will
make all terms that have any variable other than $z_n$ vanish. Thus the first
term will be a homogeneous polynomial in $z_n$, which is the principal part of
$f_i$ evaluated in $(0,z_n)$. But of course taking quotient by $z_n$ all terms
with powers of $z_n$ higher than 1 will vanish, barely leaving the principal
part of $f_i$ evaluated at $(0,1)$. But this works regardless of the coordinate
system.
\end{proof}

\section{Elementary symmetric polynomials and Newton formula}
\label{section-elementary-symmetric-polynomials-and-Newton-formula}

\noindent
The main result in this section
is to show that the elementary symmetric polynomials
can be given in terms of the Newton polynomials.
More exactly, as elements of the polynomial
ring with rational coefficients and Newton polynomials
as indeterminates.
In turn, this says that the elementary symmetric polynomials
are Weierstrass polynomials as long as the Newton polynomials
are holomorphic (which is easy to prove using the Argument Principle,
Exercise \ref{exercise-argument-principle}).
The elementary symmetric polynomials, in the form of a product,
are exactly what we get when we factor the zeroes
of a holomorphic function as in Equation \ref{equation-factor-zeroes}.
So essentially this paragraph says the proof of 
Weierstrass Preparation Theorem \ref{theorem-weierstrass-preparation-theorem}.

\medskip\noindent
The $\alpha_i$ will eventually play the roles of the zeroes
of the holomorphic function, but in this section they are just
indeterminates in the ring $\mathbb{Z}[\alpha_1,\ldots,\alpha_n]$.
We start by defining three types of polynomials in this ring.

\begin{definition}
\label{definition-symmetric-polynomial}
Let $e_i \in \mathbb{Z}[\alpha_1,\ldots,\alpha_n]$ be the coefficients of the
polynomial 
$$
t^n+e_1t^{n-1}+\ldots+e_{n-1}t+e_n:=\prod_{i=1}^n(1+\alpha_i)
$$ Then $e_i$ are called {\it elementary
symmetric polynomials} on $\alpha_i$.
\end{definition}

\noindent
Notice that $e_1=\sum_i\alpha_i=p_1$ for the Newton polynomial $p_1$ defined as
follows. Also $e_n=\prod_{i}\alpha_i$, and in general $e_k=\sum_{1\leq
i_1<\ldots<i_k\leq n}\alpha_{i_1}\ldots\alpha_{i_k}$.

\begin{definition}
\label{definition-Newton-polynomial}
A {\it Newton polynomial} is $p_j:=\sum_{i=1}^n\alpha_i^j$.
\end{definition}

\begin{definition}
\label{definition-complete-homogeneous-symmetric-polynomial}
A {\it complete homogeneous symmetric polynomial} of degree $k$ is $h_k$
obtained as a sum of all homogeneous monomials of degree $k$, that is,
$\alpha_1^k+\ldots+\alpha_n^k+\alpha_1^{k-1}\alpha_2+\ldots$.
\end{definition}

\begin{slogan}
\begin{reference}
\cite[p. 1]{generatingfunctionology}
\end{reference}
A generating function is a clothesline on which we hang up a sequence of numbers
for display.
\end{slogan}

\noindent
Corresponding to the above definitions we have the generating functions 
$E(t):=\sum_{i=0}^ne_it^i$, $P(t):=\sum_{i=1}^\infty p_it^i$ and 
$H(t):=\sum_{i=0}^\infty h_it^i$ which are formal series in 
$\mathbb{Z}[\alpha_1,\ldots,\alpha_n][\![t]\!]$.

\begin{exercise}
\label{exercise-H}
Prove that $H(t)=\prod_{i=1}^n\frac{1}{1-t\alpha_i}$.
\end{exercise}

\begin{proof}
Let us write (possibly as a formal definition)
$$
\prod_{i=1}^n\frac{1}{1-t\alpha_i}=\prod_{i=1}^n\sum_{k=0}^\infty\alpha_i^kt^k.
$$

We also write 
$f_i\overset{\text{ops}}{\longleftrightarrow}\{\alpha_i^k\}_{k=1}^\infty$ to mean
that $f_i$ is the power series associated to the sequence
$\{\alpha_i^k\}_{k=1}^\infty$. Then the equation above is the product of the
$f_i$. Then all we have to prove is that
$$
\prod_{i=1}^nf_i\overset{\text{ops}}{\longleftrightarrow}
\left\{\sum_{i_1+\ldots+i_n=k}\alpha_1^{i_1}\ldots\alpha_n^{i_n}\right\}
_{k=1}^\infty=\{h_k\}_{k=0}^\infty
$$
This is just a generalization of the formula for product of power series for a
product of $n$ power series.
\end{proof}

\begin{exercise}
\label{exercise-E}
Prove that $E(t)=\prod_{i=1}^n(1+t\alpha_i)$.
\end{exercise}

\begin{proof}
This is just writing $\prod_{i=1}^n(1+t\alpha_i)
=\prod_{i=1}^nt(\frac{1}{t}+\alpha_i)$ and continue until we get to $E(t)$.
\end{proof}

\noindent
It follows from the two previous exercises that $H(t)E(-t)=1$. Using Exercise
\ref{exercise-E} and applying logarithm we obtain that
$\frac{E'(-t)}{E(-t)}=-\sum_{i=1}^n \frac{\alpha_i}{1-t\alpha_i}$. Expanding
this formula as geometric power series we obtain that 
\begin{equation}
\label{equation-P-in-terms-of-E}
P(t)=-\frac{E'(-t)}{E(-t)}
\end{equation}

\begin{exercise}
\label{exercise-p-polynomials-of-e}
Prove that $p_i$ can be expressed as polynomials of $e_i$ (with integer
coefficients).
\end{exercise}

\begin{proof}
Using Eq. \ref{equation-P-in-terms-of-E}, and $H(t)E(-t)=1$, we have
$$
P(t)=E'(-t)H(t).
$$
Expanding the power series we obtain that the $k$-th term is
$$
p_k=\sum_{i=0}^k(-1)^{i+1}ie_ih_{k-i}
$$
\end{proof}

\begin{exercise}
\label{exercise-h-and-e}
Prove that $h_i$ can be expressed as polynomials of $e_i$ with integer
coefficients. Prove that $e_i$ can be expressed as polynomials of $h_i$ with
integer coefficients.
\end{exercise}

\begin{proof}[Sketch of proof]
By $H(t)E(-t)=1$ we see that $\frac{E'(-t)}{E(-t)}=\frac{H(t)}{H'(t)}$. Both
denominators are expressed as power series in $\alpha_i$. Multiplying by $E(-t)$
as in Exercise \ref{exercise-E} 
would let $E'(-t)$ be expressed as a power series in $h_i$ and
$\alpha_i$, while multiplying by $H(t)$ as in Exercise \ref{exercise-H} would 
let $H'(t)$ expressed in terms of
$e_i$ and $\alpha_i$.
\end{proof}

\begin{exercise}[Newton formula]
\label{exercise-Newton-formula}
Prove that $ke_k=\sum_{i=1}^{k-1}(-1)^ie_{k-i}p_i$.
\end{exercise}

\begin{proof}
Note that
$$
P(t)E(-t)=\left(\sum_{k=0}^{\infty} p_it^i\right)
\left(\sum_{i=0}^n(-1)^ie_it^i\right)
=\sum_{k=0}^\infty \sum_{i=0}^k (-1)^{k-i}e_{k-i}p_it^k
$$
using the product formula of power series, where we define $E(-t)$ as a power
series by letting $e_i=0$ for $i>n$. By Eq. \ref{equation-P-in-terms-of-E}, this
equals $E'(-t)$, which we may also see as a power series. Comparing the $k$-th
term yields the result modulo a minus sign.
\end{proof}

\noindent
Finally,

\begin{exercise}
\label{exercise-symmetric-polynomials-in-terms-of-Newton-polynomials}
Prove that $e_i$ are expressed as polynomials on $p_i$ with rational
coefficients.
\end{exercise}

\begin{proof}

\end{proof}

\section{Weierstrass preparation theorem}
\label{section-weierstrass-preparation-theorem}

\noindent
The upshot about Weierstrass preparation theorem is that
we would like it if $\mathcal{O}_n$ was an Euclidean domain,
i.e. that there was a division algorithm, 
as in Number Theory Definition \ref{number-theory-definition-euclidean-domain}.
I think that what fails has to do with the fact
that holomorphic functions are infinite series,
so that we cannot put an Euclidean function like the degree.
But instead, we have Weierstrass preparation theorem
(which gives a Weierstrass {\it division} theorem…?)

Before starting let's say the proof again.
Start with a germ of holomorphic function.
Factor its zeroes as in Equation \ref{equation-factor-zeroes}.
This gives a product of the zeroes times a holomorphic function

We denote by $B_r(z_1,\ldots,z_{n-1})$ the ball of radius $r$ in
$\mathbb{C}^{n-1}\subset\mathbb{C}^n$.

\begin{exercise}
\label{exercise-polydisks}
Let $F$ be a holomorphic function on a neighbourhood of $0$ in $\mathbb{C}^n$,
such that $\lim_{z_n\to 0} \frac{F(0,z_n)}{z_n^k}\neq 0,\infty$.
Consider the projection map $\Pi:\mathbb{C}^n\to\mathbb{C}^{n-1}$ 
$(z_1,\ldots,z_n)\mapsto (z_1,\ldots,z_{n-1})$.
\begin{enumerate}
\item Prove that for an appropriate pair $r,r'$, the resitriction of $F$ to the
polydisk $\Delta(n-1,1):=B_r(z_1,\ldots,z_{n-1})\times\Delta_{r'}(z_n)$ nowhere
vanishes on the set $\Pi^{-1}(\partial\Delta_{r'}(z_n))$.
\item Prove that in this case the resitriction of $F$ to this polydisk has
precisely $k$ zeroes $\alpha_1,\ldots,\alpha_k$ on each fiber of $\Pi$.
\item Prove that $\sum_{i=1}^k \alpha_i^d$ is a holomorphic function on
$B_r(z_1,\ldots,z_{n-1})$.
\item Prove that any elementary symmetric polynomial on $\alpha_i$ gives a
holomorphic function on $B_r(z_1,\ldots,z_{n-1})$.
\end{enumerate}
\end{exercise}

\begin{proof}
\begin{enumerate}
\item Suppose that for every $(r,r')$ there are points 
$z_1\in B_r(z_1,\ldots,z_{n-1})$ and $z_n\in \partial\Delta_{r'}(z_n)$
where $F$ vanishes. Then we obtain a convergent sequence where $F$
vanishes, and by Identity Principle \ref{theorem-identity-several-variables} we
conclude that $F$ mus be identically zero.
\item This is just applying Argument principle, i.e. the integral 
$\frac{1}{2\pi\sqrt{-1}}\int_{\partial \Delta}\frac{F'(y_0,z)}{F(y_0,z)}dz$ 
equals the number of zeroes. Since it is a continuous (holomorphic?) function on
$\Delta_y$ and integer valued, it must be constant.

The condition that $\lim_{z_n\to0}\frac{F(0,z_n)}{z_n^k}\neq 0,\infty$ means
that the fiber at 0 has a zero of order $k$:
$$
\frac{F(0,z_n)}{z_n^k}=\frac{F(0)}{z_n^k}+F'(0)\frac{z_n}{z_n^k}+\ldots
+\frac{F^{(k)}(0)}{k!}\frac{z_n^k}{z_n^k}+\ldots
$$
Thus, on other fibers we can have
more zeroes, but without multiplicities they are always $k$.
\item 
\end{enumerate}
\end{proof}

\begin{definition}
\label{definition-weierstrass-polynomial}
A {\it Weierstrass polynomial} is a function $f \in \mathcal{O}_{n-1}[z_n]$,
that is, a function which is polynomial in the last variables with coefficients
that are analytic functions on the first $n-1$ variables.
\end{definition}

\begin{exercise}
\label{exercise-weierstrass-preparation-theorem}
Let $F$ be an analytic function in a neighbourhood of $0$ in $\mathbb{C}^n$,
such that $\lim_{z_n\to 0}\neq 0,\infty$. Consider the projection map
$\Pi:\mathbb{C}^n\to\mathbb{C}^{n-1}$, 
$(z_1,\ldots,z_n)\mapsto(z_1,\ldots,z_{n-1})$, and let 
$P(z_n)\in \mathcal{O}_{n_-1}[z_n]$ be the Weierstrass polynomial given by 
$P(z_n)=\sum_{i=0}^ke_iz_n^i$, where $e_i$ are the elementary symmetric
polynomials on the zeros $\alpha_1,\ldots,\alpha_k$ defined in the previous
exercise. Prove that $F=P(z_n)u$, where $u$ is a germ of an invertible
holomorphic function.
\end{exercise}

\begin{proof}
For every $y_0$ we can write
$$
F(y_0,z_n)=(z_n-\alpha_1)\ldots(z_n-\alpha_n)h(z_n)
$$
where implicitly all $\alpha_i$ and $h$ depend on $y_0$. The product of 
$(z_n-\alpha_i)$ is the definition of $P$. Varying $y_0$ we obtain the result.
\end{proof}

This argument works for every function that has a zero of order $k$ in
$0\in\mathbb{C}^n$.

As I recall the following is \cite{GH} formulation of the theorem:

\begin{theorem}[Weierstrass preparation theorem]
If $f:U\subset\mathbb{C}^n\to\mathbb{C}$ is holomorphic and $f$ is not
identically zero in the coordinate axis $z_n:=w$, there
is a unique germ of a monic Weierstrass polynomial $g$ whose coefficients are
holomorphic functions on the first $n-1$ variables 
and a germ of a holomorphic 
function $h$ with $h(0)\neq 0$ 
(i.e. $h$ is a unit of $\mathcal{O}_n$)
such that $f=gh$.
\end{theorem}

\noindent
But I will stick to the formulation in \cite{Demailly}:

\begin{theorem}[Weierstrass preparation theorem]
\label{theorem-weierstrass-preparation-theorem}
Let $g$ be holomorphic on a neighourhood of $0$ in $\mathbb{C}^n$,
such that $g(0,z_n)/z_n^s$ has a nonzero finite limit at $z_n=0$.
With the ``above'' choice of $r'$ and $r_n$,
one can write 
$$
g(z)=u(z)P(z',z_n)
$$
where $u$ is an invertible holomorphic function
in a neighbourhood of the polydisk $\overline{\Delta}(r',r_n)$ 
and $P$ is a Weierstrass polynomial with coefficient
functions defined for $|z'|\leq r'$ in $\mathbb{C}^{n-1}$.
\end{theorem}

\begin{proof}
We do the proof in steps

\medskip\noindent
Step 0. First notice that since I will use this theorem
to prove that $\mathcal{O}_n$ is a UFD, I will start with
an arbitrary function $g \in \mathcal{O}_n$.
(Right now it's not clear why we need that $g(0)=0$.)
But then it's straightforward to change coordinates so that $g$
is not identically zero on the  $z_n$-axis:
just complete the point where $g$ is nonzero to a basis $\{b_i\}_{i=1}^{n-1}$
and precompose with a linear map sending to $(0,\ldots,0,1)$
to that point, and the rest of the canonical basis
to the corresponding basis vectors $b_i$.
The composition of this linear map with $g$ remains holomorphic
and is nonvanishing in the $z_n$-axis.

(See Exercise \ref{exercise-principal-parts} for a proof that we can suppose
that the coordinate change preserves the order of the zero,
but again it's not clear why I need that.)

\medskip\noindent
Step 1. Now we define the correct domain;
skip this for now until it comes into play.

Then $g(0,z_n)$ is a holomorphic function with a zero on $0$. By
the Taylor expansion we know that this zero must be isolated, so that there is a
number $r_n>0$ such that $f(0,z_n)\neq 0$ for  $0<|z_n|\leq r_n$.

Since the circle $|z_n|=r_n$ is compact and $f(0,z_n)\neq 0$ on this circle, we
can fix $r'>0$ such that $f(z',z_n)\neq 0$ for $z'<r'$ and $|z_n|<
r_n+\varepsilon$.

\medskip\noindent
Step 2. Now we apply Argument principle. 
By Exercise
\ref{exercise-argument-principle}, we know that 
$S_0=\frac{1}{2\pi\sqrt{-1}}\int\frac{f'(z',z)}{f(z',z)}dz$ counts the number of
zeroes of $f(z',-)$ on $\Delta(r_n)$ for fixed $z' \in \Delta(r')$.

Notice that the map $S_0$ is holomorphic as a function of $z'$ just because the
complex derivative, defined as a limit, can go into the integral.
%giving a
%well-defined complex number; thus we can differentiate infinitely many times and
%obtain a Taylor expansion.
Moreover, since
$S_0(z')$ is an integer,
it's constant (as a function of $z'$) by continuity.
Thus we have exactly $s$ functions
$w_1(z'),\ldots,w_s(z')$ which give the $s$ zeroes of $f(z',-)$
at every $z' \in \Delta(r')$. 
Now we apply argument principle with multiplication by
$z^k$ to obtain $S_k(z')=\sum w_i(z')^k$ (without multiplicities).

\medskip\noindent
Step 3. Now we use elementary symmetric polynomials.
We can think of
$S_k(z')=\sum_{i=1}^sw_i^k=p_k$ as the Newton polynomials in the
indeterminates $w_i$.
In the last step we showed they are holomorphic functions
of $z'$. 
In Exercise \ref{exercise-symmetric-polynomials-in-terms-of-Newton-polynomials}
we showed that the elementary symmetric polynomials $e_i$ can be expressed 
as polynomials of $p_i$ with rational coefficients,
that is, $e_i\in\mathbb{Q}[p_1,\ldots,p_k]$.
We conclude that the $e_i$ are also holomorphic functions of $z'$. 
This shows that
\begin{align*}
P(z',z_n):&=\prod_{i=1}^k(z_n-w_i(z'))\\
&=t^n+e_1t^{n-1}+\ldots+e_{n-1}t+e_n,
\end{align*}

\noindent
where the $e_i$ are the elementary symmetric polynomials,
defined exactly by the above relation (see Section
\ref{section-elementary-symmetric-polynomials-and-Newton-formula}),
is a Weiestrass polynomial 
(because its coefficients are holomorphic functions).

\medskip\noindent
Step 4. Just apply Cauchy formula to figure out
the final equation. 
Factor the zeroes of $f(z',-)$ as in  Equation \ref{equation-factor-zeroes}
to obtain
$f(z',z_n)=\left(\prod_{i}z_n-w_i(z')\right)h_{z'}(z_n):=Ph$.

The classical way is to notice that the function $f/P$ is holomorphic in $z_n$
because $f$ and $P$ have the same zeroes, and we write the integral formula to
complete to a holomorphic function (but I think it should be enough with my
argument).
\end{proof}

\noindent
Recall from Number Theory Lemma \ref{lemma-gauss}
that if $R$ is a UFD, then $R[x]$ is a UFD.

\begin{lemma}
\label{lemma-stalk-is-UFD}
The stalk $\mathcal{O}_n:=\mathcal{O}_{\mathbb{C}^n,0}$ is a UFD.
\end{lemma}

\begin{proof}
By induction on $n$. For $n=0$ it is trivial. Suppose $\mathcal{O}_{n-1}$ is a
UFD. Then by Gauss' Lemma \ref{number-theory-lemma-gauss}, 
$\mathcal{O}_{n-1}[w]$ is a UFD
too. Thus we may express any Weierstrass polynomial $g$ as a product of
irreducible elements (uniquely up to multiplication by units).

Let $f\in \mathcal{O}_n$. We want to express $f$ as a product of (unique up to
multiplication by units) of irreducible elements. By Weierstrass Preparation
Theorem \ref{theorem-weierstrass-preparation} there is a Weierstrass polynomial
$g\in\mathcal{O}_n[w]$ and a holomorphic function not vanishing on $0$ (i.e. a
unit of $\mathcal{O}_n$) such that $f=gh$. By the previous remark $g$ is
factored uniquely up to multiplication by units as $g=g_1\ldots g_m$. This shows
existence of the factorization.

To prove uniqueness suppose that $f=f_1\ldots f_k$ for some irreducible
$f_1,\ldots,f_k\in\mathcal{O}_n$. Since $f$ does not vanish in the $w$ axis,
neither can each $f_i$, so that we may decompose each of them as  $f_i=g_i'h_i$
by Weierstrass Preparation Theorem. Since $f_i$ is irreducible, it follows that
$g_i'$ is irreducible. Then we have that $$ f=gh=\prod g_i'\prod h_i $$ so by
uniqueness in Weierstrass Preparation Theorem we conclude that $g=\prod g_i'$,
and by uniqueness from the fact that  $\mathcal{O}_n[w]$ is a UFD we conclude
that $g$ coincides with $\prod g_i'$ up to multiplication by units.
\end{proof}

\bibliography{my}
\bibliographystyle{amsalpha}

\end{document}
